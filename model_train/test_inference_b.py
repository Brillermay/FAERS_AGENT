import os
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

import torch
import pickle
import numpy as np
from transformers import AutoTokenizer
from train_strategy_b import RetrievalReactionModel
from typing import List, Tuple


class RetrievalReactionPredictor:
    """æ£€ç´¢å¼æ¨¡å‹æ¨ç†å™¨"""
    
    def __init__(self, model_dir='/home/motao/project/strategy_retrieval_output/final_model'):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ£€ç´¢å¼æ¨¡å‹
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆåŒ…å« final_model æ–‡ä»¶å¤¹ï¼‰
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“‚ ä» {model_dir} åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½å…ƒæ•°æ®
        parent_dir = os.path.dirname(model_dir)  # ä¸Šçº§ç›®å½•
        metadata_path = os.path.join(parent_dir, 'metadata.pkl')
        
        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
        
        with open(metadata_path, 'rb') as f:
            metadata = pickle.load(f)
        
        self.mlb = metadata['mlb']
        self.all_reaction_names = metadata['all_reaction_names']
        self.config = metadata['config']
        
        print(f"ğŸ“Š ååº”ç±»åˆ«æ•°: {len(self.all_reaction_names)}")
        print(f"ğŸ·ï¸  å‰5ä¸ªååº”: {self.all_reaction_names[:5]}")
        
        # åŠ è½½ tokenizer
        model_name = self.config['model_name']
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"ğŸ“¦ Tokenizer åŠ è½½å®Œæˆ: {model_name}")
        
        # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
        self.model = RetrievalReactionModel(
            model_name=model_name,
            num_labels=len(self.all_reaction_names),
            reaction_names=self.all_reaction_names,
            dropout_rate=self.config.get('dropout_rate', 0.1)
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        weight_files = [
            'model.safetensors',
            'pytorch_model.bin',
            'model.bin',
        ]
        
        loaded = False
        for weight_file in weight_files:
            weight_path = os.path.join(model_dir, weight_file)
            if os.path.exists(weight_path):
                print(f"ğŸ“¥ åŠ è½½æƒé‡: {weight_file}")
                
                if weight_file.endswith('.safetensors'):
                    try:
                        from safetensors.torch import load_file
                        state_dict = load_file(weight_path)
                    except ImportError:
                        print("âš ï¸  safetensors æœªå®‰è£…ï¼Œä½¿ç”¨ torch åŠ è½½...")
                        state_dict = torch.load(weight_path, map_location=self.device)
                else:
                    state_dict = torch.load(weight_path, map_location=self.device)
                
                self.model.load_state_dict(state_dict, strict=False)
                loaded = True
                break
        
        if not loaded:
            actual_files = os.listdir(model_dir) if os.path.exists(model_dir) else []
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œç›®å½•å†…å®¹: {actual_files}")
        
        self.model.to(self.device)
        self.model.eval()
        
        # é¢„è®¡ç®—æ‰€æœ‰ååº”çš„å‘é‡ï¼ˆç”¨äºå¿«é€Ÿæ¨ç†ï¼‰
        print("ğŸ”§ é¢„è®¡ç®—ååº”å‘é‡...")
        self._precompute_reaction_embeddings()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    
    def _precompute_reaction_embeddings(self, batch_size=32):
        """
        é¢„è®¡ç®—æ‰€æœ‰700ä¸ªååº”çš„å‘é‡è¡¨ç¤º
        æ¨ç†æ—¶åªéœ€ç¼–ç ç—…ä¾‹ï¼Œç„¶ååšå‘é‡ç‚¹ç§¯
        """
        all_vecs = []
        
        with torch.no_grad():
            for i in range(0, len(self.all_reaction_names), batch_size):
                batch_reactions = self.all_reaction_names[i:i+batch_size]
                
                # Tokenize
                encoding = self.tokenizer(
                    batch_reactions,
                    truncation=True,
                    padding=True,
                    max_length=64,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(self.device)
                attention_mask = encoding['attention_mask'].to(self.device)
                
                # ç¼–ç 
                vecs = self.model.encode_reaction(input_ids, attention_mask)
                all_vecs.append(vecs.cpu())
        
        # æ‹¼æ¥æ‰€æœ‰å‘é‡: (num_labels, 256)
        self.reaction_embeddings = torch.cat(all_vecs, dim=0)
        print(f"âœ… ååº”å‘é‡çŸ©é˜µ: {self.reaction_embeddings.shape}")
    
    def predict(self, prompt: str, top_k: int = 10, return_scores: bool = True) -> List[Tuple[str, float]]:
        """
        å¯¹å•ä¸ªç—…ä¾‹è¿›è¡Œé¢„æµ‹
        
        Args:
            prompt: ç—…ä¾‹æè¿°æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªé¢„æµ‹
            return_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            [(ååº”åç§°, ç›¸ä¼¼åº¦åˆ†æ•°), ...] æˆ– [ååº”åç§°, ...]
        """
        # Tokenize ç—…ä¾‹
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        with torch.no_grad():
            # ç¼–ç ç—…ä¾‹: (1, 256)
            case_vec = self.model.encode_case(input_ids, attention_mask)
            
            # è®¡ç®—ä¸æ‰€æœ‰ååº”çš„ç›¸ä¼¼åº¦: (1, num_labels)
            scores = torch.mm(case_vec, self.reaction_embeddings.T.to(self.device))
            scores = scores.cpu().numpy()[0]  # (num_labels,)
        
        # æ’åºè·å– Top-K
        top_indices = np.argsort(-scores)[:top_k]
        
        results = []
        for idx in top_indices:
            reaction_name = self.all_reaction_names[idx]
            score = float(scores[idx])
            
            if return_scores:
                results.append((reaction_name, score))
            else:
                results.append(reaction_name)
        
        return results
    
    def predict_batch(self, prompts: List[str], top_k: int = 10, 
                     batch_size: int = 16, return_scores: bool = True) -> List[List[Tuple[str, float]]]:
        """
        æ‰¹é‡é¢„æµ‹ï¼ˆæ›´é«˜æ•ˆï¼‰
        
        Args:
            prompts: ç—…ä¾‹åˆ—è¡¨
            top_k: æ¯ä¸ªç—…ä¾‹è¿”å›å‰Kä¸ªé¢„æµ‹
            batch_size: æ‰¹å¤„ç†å¤§å°
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°
            
        Returns:
            æ¯ä¸ªç—…ä¾‹çš„é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        with torch.no_grad():
            for i in range(0, len(prompts), batch_size):
                batch_prompts = prompts[i:i+batch_size]
                
                # Tokenize
                encoding = self.tokenizer(
                    batch_prompts,
                    truncation=True,
                    padding=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(self.device)
                attention_mask = encoding['attention_mask'].to(self.device)
                
                # ç¼–ç ç—…ä¾‹: (B, 256)
                case_vecs = self.model.encode_case(input_ids, attention_mask)
                
                # è®¡ç®—ç›¸ä¼¼åº¦: (B, num_labels)
                scores = torch.mm(case_vecs, self.reaction_embeddings.T.to(self.device))
                scores = scores.cpu().numpy()
                
                # å¯¹æ¯ä¸ªæ ·æœ¬æå– Top-K
                for sample_scores in scores:
                    top_indices = np.argsort(-sample_scores)[:top_k]
                    
                    sample_results = []
                    for idx in top_indices:
                        reaction_name = self.all_reaction_names[idx]
                        score = float(sample_scores[idx])
                        
                        if return_scores:
                            sample_results.append((reaction_name, score))
                        else:
                            sample_results.append(reaction_name)
                    
                    all_results.append(sample_results)
        
        return all_results
    
    def explain_prediction(self, prompt: str, reaction_name: str) -> dict:
        """
        è§£é‡Šä¸ºä»€ä¹ˆé¢„æµ‹æŸä¸ªååº”ï¼ˆè¿”å›ç›¸ä¼¼åº¦åˆ†æ•°å’Œæ’åï¼‰
        
        Args:
            prompt: ç—…ä¾‹æè¿°
            reaction_name: è¦è§£é‡Šçš„ååº”åç§°
            
        Returns:
            {'score': ç›¸ä¼¼åº¦åˆ†æ•°, 'rank': æ’å, 'percentile': ç™¾åˆ†ä½}
        """
        if reaction_name not in self.all_reaction_names:
            raise ValueError(f"ååº” '{reaction_name}' ä¸åœ¨æ¨¡å‹è¯è¡¨ä¸­")
        
        # è·å–æ‰€æœ‰ç›¸ä¼¼åº¦
        predictions = self.predict(prompt, top_k=len(self.all_reaction_names), return_scores=True)
        
        # æŸ¥æ‰¾ç›®æ ‡ååº”
        for rank, (pred_reaction, score) in enumerate(predictions, 1):
            if pred_reaction == reaction_name:
                percentile = (1 - rank / len(self.all_reaction_names)) * 100
                return {
                    'score': score,
                    'rank': rank,
                    'percentile': percentile,
                    'total_reactions': len(self.all_reaction_names)
                }
        
        return {'error': f'æœªæ‰¾åˆ°ååº” {reaction_name}'}


def main():
    """æµ‹è¯•æ¨ç†"""
    print("=" * 60)
    print("ğŸ¯ æ£€ç´¢å¼æ¨¡å‹æ¨ç†æµ‹è¯•")
    print("=" * 60 + "\n")
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = RetrievalReactionPredictor()
    
    # æµ‹è¯•ç—…ä¾‹
    test_prompt = """[PAT] age: 80 sex: M wt: 95.0kg country: GB season: spring [/PAT]
[INDI] Ill-defined disorder [/INDI]
[DRUGS_FOR_INDI]
[DRUG] ARICEPT | ai: DONEPEZIL HYDROCHLORIDE | dose: 5.0 mg | freq: daily [/DRUG]
[DRUG] AMLODIPINE | ai: AMLODIPINE BESYLATE | dose: unknown | freq: daily [/DRUG]
[DRUG] ATORVASTATIN | ai: ATORVASTATIN | dose: unknown | freq: daily [/DRUG]
[DRUG] BISOPROLOL | ai: BISOPROLOL | dose: unknown | freq: daily [/DRUG]
[/DRUGS_FOR_INDI]"""
    
    # æµ‹è¯•1: å•ä¸ªé¢„æµ‹
    print("ğŸ“‹ æµ‹è¯•ç—…ä¾‹:")
    print("-" * 60)
    print(test_prompt[:200] + "...")
    print("-" * 60 + "\n")
    
    print("ğŸ” é¢„æµ‹ä¸­...\n")
    predictions = predictor.predict(test_prompt, top_k=10)
    
    print("ğŸ“Š Top-10 é¢„æµ‹ç»“æœ:")
    print("=" * 60)
    for i, (reaction, score) in enumerate(predictions, 1):
        # ç›¸ä¼¼åº¦å¾—åˆ†èŒƒå›´é€šå¸¸åœ¨ [-1, 1]ï¼Œå½’ä¸€åŒ–åˆ° [0, 100]
        confidence = (score + 1) / 2 * 100
        print(f"{i:2d}. {reaction:45s} | ç›¸ä¼¼åº¦: {score:6.4f} ({confidence:5.2f}%)")
    print("=" * 60 + "\n")
    
    # æµ‹è¯•2: æ‰¹é‡é¢„æµ‹
    print("ğŸ“¦ æ‰¹é‡é¢„æµ‹æµ‹è¯•...")
    batch_prompts = [test_prompt] * 3  # å¤åˆ¶3ä»½æµ‹è¯•
    batch_results = predictor.predict_batch(batch_prompts, top_k=5, batch_size=2)
    
    print(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆï¼Œå¤„ç† {len(batch_results)} ä¸ªæ ·æœ¬")
    print(f"   æ ·æœ¬1çš„Top-5: {[r[0] for r in batch_results[0]]}\n")
    
    # æµ‹è¯•3: è§£é‡Šé¢„æµ‹
    if predictions:
        target_reaction = predictions[0][0]  # ç¬¬ä¸€ä¸ªé¢„æµ‹
        print(f"ğŸ” è§£é‡Šé¢„æµ‹: ä¸ºä»€ä¹ˆé¢„æµ‹ '{target_reaction}'?")
        explanation = predictor.explain_prediction(test_prompt, target_reaction)
        print(f"   - ç›¸ä¼¼åº¦åˆ†æ•°: {explanation['score']:.4f}")
        print(f"   - æ’å: {explanation['rank']}/{explanation['total_reactions']}")
        print(f"   - ç™¾åˆ†ä½: {explanation['percentile']:.2f}%")
        print()
    
    # æ€§èƒ½ç»Ÿè®¡
    print("ğŸ“ˆ æ¨¡å‹ä¿¡æ¯:")
    print(f"   - æ€»ååº”æ•°: {len(predictor.all_reaction_names)}")
    print(f"   - å‘é‡ç»´åº¦: {predictor.reaction_embeddings.shape[1]}")
    print(f"   - è®¾å¤‡: {predictor.device}")
    print()


if __name__ == "__main__":
    main()