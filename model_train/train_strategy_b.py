import os
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from transformers import (
    AutoTokenizer, AutoModel, 
    Trainer, TrainingArguments,
)
from torch.utils.data import Dataset
import pickle
import json
from tqdm import tqdm
import random
from data_preparation import prepare_data_strategy_b


# ============= ğŸ”¥ æ ¸å¿ƒ1: äº¤å‰æ³¨æ„åŠ›æ¨¡å‹æ¶æ„ =============
class CrossAttentionReactionModel(nn.Module):
    """
    äº¤å‰æ³¨æ„åŠ›æ¶æ„:
    1. ç¼–ç ç—…ä¾‹ (æ‚£è€…+é€‚åº”ç—‡+è¯ç‰©)
    2. ç¼–ç æ‰€æœ‰ååº”åç§° (é¢„è®¡ç®—)
    3. äº¤å‰æ³¨æ„åŠ›: ç—…ä¾‹å…³æ³¨ååº”,å­¦ä¹ å…³è”
    4. åˆ†ç±»å¤´è¾“å‡ºæ‰€æœ‰ååº”çš„æ¦‚ç‡
    """
    def __init__(self, model_name, num_labels, reaction_names, dropout_rate=0.1):
        super().__init__()
        
        # BERTç¼–ç å™¨
        self.bert = AutoModel.from_pretrained(model_name)
        hidden_size = self.bert.config.hidden_size  # é€šå¸¸æ˜¯768
        
        self.num_labels = num_labels
        self.reaction_names = reaction_names
        
        # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›1: å¤šå¤´äº¤å‰æ³¨æ„åŠ›å±‚
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=dropout_rate,
            batch_first=True
        )
        
        # ğŸ”¥ æ ¸å¿ƒæ”¹è¿›2: ååº”æ„ŸçŸ¥çš„åˆ†ç±»å¤´
        # è¾“å…¥: ç—…ä¾‹è¡¨ç¤º + æ³¨æ„åŠ›è¾“å‡º (æ‹¼æ¥)
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size * 2, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, num_labels)  # ç›´æ¥è¾“å‡º700ä¸ªååº”çš„logits
        )
        
        # ğŸ”¥ é¢„è®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰ååº”çš„åµŒå…¥
        self.register_buffer('reaction_embeddings', torch.zeros(num_labels, hidden_size))
        self._initialized = False
        
    def initialize_reaction_embeddings(self, tokenizer, device, batch_size=32):
        """
        é¢„å…ˆç¼–ç æ‰€æœ‰700ä¸ªååº”åç§°
        åªéœ€åœ¨è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨ä¸€æ¬¡
        """
        if self._initialized:
            return
        
        print("ğŸ”§ é¢„è®¡ç®—ååº”åµŒå…¥...")
        self.eval()
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in tqdm(range(0, len(self.reaction_names), batch_size), desc="ç¼–ç ååº”"):
                batch_reactions = self.reaction_names[i:i+batch_size]
                
                encoding = tokenizer(
                    batch_reactions,
                    truncation=True,
                    padding=True,
                    max_length=64,
                    return_tensors='pt'
                )
                
                input_ids = encoding['input_ids'].to(device)
                attention_mask = encoding['attention_mask'].to(device)
                
                outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
                embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] token
                all_embeddings.append(embeddings.cpu())
        
        self.reaction_embeddings = torch.cat(all_embeddings, dim=0).to(device)
        self._initialized = True
        print(f"âœ… ååº”åµŒå…¥çŸ©é˜µ: {self.reaction_embeddings.shape}")
    
    def forward(self, input_ids, attention_mask, labels=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: (B, seq_len) ç—…ä¾‹æ–‡æœ¬
            attention_mask: (B, seq_len)
            labels: (B, num_labels) å¤šæ ‡ç­¾ç›®æ ‡ [0æˆ–1]
        
        Returns:
            dict: {'loss': loss, 'logits': logits}
        """
        batch_size = input_ids.shape[0]
        device = input_ids.device
        
        # Step 1: ç¼–ç ç—…ä¾‹
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # (B, seq_len, hidden_size)
        case_cls = sequence_output[:, 0, :]  # (B, hidden_size)
        
        # Step 2: äº¤å‰æ³¨æ„åŠ› - ç—…ä¾‹å¦‚ä½•å…³æ³¨æ¯ä¸ªååº”
        # query: ç—…ä¾‹çš„[CLS]è¡¨ç¤º
        # key/value: æ‰€æœ‰ååº”çš„åµŒå…¥
        
        query = case_cls.unsqueeze(1)  # (B, 1, hidden_size)
        
        # æ‰©å±•ååº”åµŒå…¥åˆ°batchç»´åº¦
        reaction_emb = self.reaction_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  
        # (B, num_labels, hidden_size)
        
        # ğŸ”¥ äº¤å‰æ³¨æ„åŠ›è®¡ç®—
        attended_output, attention_weights = self.cross_attention(
            query=query,           # (B, 1, hidden_size) - ç—…ä¾‹è¯¢é—®
            key=reaction_emb,      # (B, num_labels, hidden_size) - ååº”ä½œä¸ºkey
            value=reaction_emb     # (B, num_labels, hidden_size) - ååº”ä½œä¸ºvalue
        )
        # attended_output: (B, 1, hidden_size) - åŠ æƒèåˆçš„ååº”ä¿¡æ¯
        attended_output = attended_output.squeeze(1)  # (B, hidden_size)
        
        # Step 3: æ‹¼æ¥åŸå§‹ç—…ä¾‹è¡¨ç¤ºå’Œæ³¨æ„åŠ›è¾“å‡º
        combined = torch.cat([case_cls, attended_output], dim=1)  # (B, hidden_size * 2)
        
        # Step 4: åˆ†ç±»å¾—åˆ°æ‰€æœ‰ååº”çš„logits
        logits = self.classifier(combined)  # (B, num_labels)
        
        # Step 5: è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            loss_fct = FocalLoss(alpha=0.25, gamma=2.0)
            loss = loss_fct(logits, labels)
        
        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}


# ============= ğŸ”¥ æ ¸å¿ƒ2: Focal Losså®ç° =============
class FocalLoss(nn.Module):
    """
    Focal Loss for Multi-label Classification
    
    FL(p_t) = -Î±(1-p_t)^Î³ * log(p_t)
    
    - Î±: å¹³è¡¡æ­£è´Ÿæ ·æœ¬æƒé‡
    - Î³: è°ƒèŠ‚éš¾æ˜“æ ·æœ¬æƒé‡ (Î³è¶Šå¤§,ç®€å•æ ·æœ¬æƒé‡è¶Šä½)
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, logits, labels):
        """
        Args:
            logits: (B, num_labels) - æ¨¡å‹è¾“å‡º
            labels: (B, num_labels) - äºŒå€¼æ ‡ç­¾ [0æˆ–1]
        """
        # è®¡ç®—æ¦‚ç‡
        probs = torch.sigmoid(logits)
        
        # ğŸ”¥ æ ¸å¿ƒå…¬å¼: p_t = p if y==1 else 1-p
        p_t = probs * labels + (1 - probs) * (1 - labels)
        
        # ğŸ”¥ Focal weight: (1 - p_t)^gamma
        # å½“p_tæ¥è¿‘1(ç®€å•æ ·æœ¬),æƒé‡æ¥è¿‘0
        # å½“p_tæ¥è¿‘0(éš¾æ ·æœ¬),æƒé‡æ¥è¿‘1
        focal_weight = (1 - p_t) ** self.gamma
        
        # BCE loss
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, labels, reduction='none'
        )
        
        # ğŸ”¥ æœ€ç»ˆæŸå¤± = alpha * focal_weight * bce_loss
        focal_loss = self.alpha * focal_weight * bce_loss
        
        return focal_loss.mean()


# ============= ğŸ”¥ æ ¸å¿ƒ3: ç®€åŒ–çš„å¤šæ ‡ç­¾æ•°æ®é›† =============
class MultiLabelDataset(Dataset):
    """
    ä¸éœ€è¦è´Ÿé‡‡æ ·,ç›´æ¥è¿”å›å¤šæ ‡ç­¾å‘é‡
    """
    def __init__(self, prompts, label_indices, num_labels):
        """
        Args:
            prompts: ç—…ä¾‹æ–‡æœ¬åˆ—è¡¨
            label_indices: æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾ç´¢å¼•åˆ—è¡¨
            num_labels: æ€»æ ‡ç­¾æ•° (700)
        """
        self.prompts = prompts
        self.label_indices = label_indices
        self.num_labels = num_labels
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        prompt = self.prompts[idx]
        labels = self.label_indices[idx]
        
        # è½¬æ¢ä¸ºå¤šæ ‡ç­¾å‘é‡
        label_vec = np.zeros(self.num_labels, dtype=np.float32)
        
        if labels is not None and len(labels) > 0:
            # å…¼å®¹ä¸åŒæ ¼å¼
            labels = list(labels) if not isinstance(labels, list) else labels
            valid_labels = [i for i in labels if 0 <= i < self.num_labels]
            
            if valid_labels:
                label_vec[valid_labels] = 1.0
        
        return {
            'prompt': prompt,
            'labels': label_vec
        }


# ============= ğŸ”¥ æ ¸å¿ƒ4: ç®€åŒ–çš„Collator =============
def multilabel_collate_fn(batch, tokenizer, max_length=512):
    """
    å°†batchè½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
    """
    prompts = [b['prompt'] for b in batch]
    labels = torch.tensor([b['labels'] for b in batch], dtype=torch.float32)
    
    # Tokenizeç—…ä¾‹
    encoding = tokenizer(
        prompts,
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    return {
        'input_ids': encoding['input_ids'],
        'attention_mask': encoding['attention_mask'],
        'labels': labels
    }


# ============= ğŸ”¥ æ ¸å¿ƒ5: æ ‡å‡†Trainer =============
class MultiLabelTrainer(Trainer):
    """ä½¿ç”¨æ ‡å‡†çš„Trainer,ä¸éœ€è¦ç‰¹æ®Šå¤„ç†"""
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs['loss']
        return (loss, outputs) if return_outputs else loss


# ============= ğŸ”¥ æ ¸å¿ƒ6: è¯„ä¼°æŒ‡æ ‡ =============
def compute_multilabel_metrics(eval_pred, all_reaction_names):
    """
    è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡
    
    Args:
        eval_pred: EvalPredictionå¯¹è±¡
        all_reaction_names: æ‰€æœ‰ååº”åç§°åˆ—è¡¨
    """
    logits = eval_pred.predictions
    labels = eval_pred.label_ids
    
    # Sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
    probs = 1 / (1 + np.exp(-logits))
    
    # è®¡ç®—ä¸åŒKå€¼çš„æŒ‡æ ‡
    k_values = [1, 3, 5, 10, 20]
    metrics = {}
    
    for k in k_values:
        # è·å–Top-Ké¢„æµ‹
        top_k_indices = np.argsort(-probs, axis=1)[:, :k]
        
        recalls = []
        precisions = []
        f1s = []
        
        for i in range(len(labels)):
            true_labels = set(np.where(labels[i] == 1)[0])
            pred_labels = set(top_k_indices[i])
            
            if len(true_labels) == 0:
                continue
            
            # Recall@K
            recall = len(true_labels & pred_labels) / len(true_labels)
            recalls.append(recall)
            
            # Precision@K
            precision = len(true_labels & pred_labels) / k if k > 0 else 0
            precisions.append(precision)
            
            # F1@K
            if recall + precision > 0:
                f1 = 2 * recall * precision / (recall + precision)
                f1s.append(f1)
        
        metrics[f'recall_at_{k}'] = np.mean(recalls) if recalls else 0.0
        metrics[f'precision_at_{k}'] = np.mean(precisions) if precisions else 0.0
        metrics[f'f1_at_{k}'] = np.mean(f1s) if f1s else 0.0
    
    # è®¡ç®—MRR
    mrrs = []
    for i in range(len(labels)):
        true_labels = set(np.where(labels[i] == 1)[0])
        if len(true_labels) == 0:
            continue
        
        sorted_indices = np.argsort(-probs[i])
        for rank, idx in enumerate(sorted_indices, 1):
            if idx in true_labels:
                mrrs.append(1.0 / rank)
                break
    
    metrics['mrr'] = np.mean(mrrs) if mrrs else 0.0
    
    return metrics


# ============= é…ç½® =============
CROSSATT_CONFIG = {
    'csv_path': './outputs/prompts_sample_10000_detailed_detailed_v2.csv',
    'max_samples': 10000,
    'high_freq_min': 75,
    'med_freq_min': 10,
    'low_freq_min': 5,
    'high_ratio': 0.6,
    'med_ratio': 0.9,
    'low_ratio': 0.3,
    
    'model_name': '/home/motao/project/models/Bio_ClinicalBERT_local',
    'max_length': 512,
    'dropout_rate': 0.15,
    
    'epochs': 8,
    'batch_size': 8,  # å¯ä»¥æ›´å¤§,å› ä¸ºä¸éœ€è¦è´Ÿé‡‡æ ·
    'learning_rate': 3e-5,
    'warmup_ratio': 0.1,
    'weight_decay': 0.01,
    'gradient_accumulation_steps': 2,
    
    'output_dir': '/home/motao/project/strategy_crossatt_output',
}


# ============= ä¸»è®­ç»ƒç±» =============
class CrossAttentionTrainer:
    def __init__(self, config):
        self.config = config
        os.makedirs(config['output_dir'], exist_ok=True)
    
    def prepare_data(self):
        print("ğŸ”§ å‡†å¤‡æ•°æ®...")
        
        train_data, val_data, mlb, selected_reactions = prepare_data_strategy_b(
            self.config['csv_path'],
            max_samples=self.config['max_samples'],
            high_freq_min=self.config['high_freq_min'],
            med_freq_min=self.config['med_freq_min'],
            low_freq_min=self.config['low_freq_min'],
            high_ratio=self.config['high_ratio'],
            med_ratio=self.config['med_ratio'],
            low_ratio=self.config['low_ratio']
        )
        
        self.train_data = train_data
        self.val_data = val_data
        self.mlb = mlb
        self.all_reaction_names = list(mlb.classes_)
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ - ååº”æ•°: {len(self.all_reaction_names)}")
    
    def create_datasets(self):
        print("ğŸ”§ åˆ›å»ºæ•°æ®é›†...")
        
        if isinstance(self.train_data, dict):
            train_prompts = self.train_data.get('prompts', self.train_data.get('texts', []))
            train_labels = self.train_data.get('labels', self.train_data.get('label_indices', []))
            
            val_prompts = self.val_data.get('prompts', self.val_data.get('texts', []))
            val_labels = self.val_data.get('labels', self.val_data.get('label_indices', []))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼")
        
        self.train_dataset = MultiLabelDataset(
            train_prompts, train_labels, len(self.all_reaction_names)
        )
        
        self.val_dataset = MultiLabelDataset(
            val_prompts, val_labels, len(self.all_reaction_names)
        )
        
        print(f"âœ… è®­ç»ƒé›†: {len(self.train_dataset)}, éªŒè¯é›†: {len(self.val_dataset)}")
    
    def train(self):
        print("ğŸš€ å¼€å§‹äº¤å‰æ³¨æ„åŠ›è®­ç»ƒ...")
        
        self.prepare_data()
        self.create_datasets()
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = CrossAttentionReactionModel(
            model_name=self.config['model_name'],
            num_labels=len(self.all_reaction_names),
            reaction_names=self.all_reaction_names,
            dropout_rate=self.config['dropout_rate']
        )
        
        # ğŸ”¥ é¢„è®¡ç®—ååº”åµŒå…¥
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.initialize_reaction_embeddings(self.tokenizer, device)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config['output_dir'],
            num_train_epochs=self.config['epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'] * 2,
            learning_rate=self.config['learning_rate'],
            warmup_ratio=self.config['warmup_ratio'],
            weight_decay=self.config['weight_decay'],
            logging_steps=50,
            save_strategy="epoch",
            save_total_limit=2,
            eval_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="recall_at_5",
            greater_is_better=True,
            report_to=None,
            fp16=True,
            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
            remove_unused_columns=False,
        )
        
        def data_collator(batch):
            return multilabel_collate_fn(batch, self.tokenizer, self.config['max_length'])
        
        # ğŸ”¥ ä½¿ç”¨æ ‡å‡†Trainer
        trainer = MultiLabelTrainer(
            model=model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.val_dataset,
            data_collator=data_collator,
            compute_metrics=lambda p: compute_multilabel_metrics(p, self.all_reaction_names),
        )
        
        print("è®­ç»ƒå¼€å§‹...")
        trainer.train()
        
        print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        trainer.save_model(os.path.join(self.config['output_dir'], "final_model"))
        
        print("\nğŸ¯ æœ€ç»ˆè¯„ä¼°...")
        final_metrics = trainer.evaluate()
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\n" + "="*50)
        print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ")
        print("="*50)
        
        print("\nğŸ¯ Recall@K:")
        for k in [1, 3, 5, 10, 15]:
            print(f"  Recall@{k:2d}: {final_metrics.get(f'eval_recall_at_{k}', 0):.4f}")
        
        print("\nğŸ“Œ Precision@K:")
        for k in [1, 3, 5, 10, 15]:
            print(f"  Precision@{k:2d}: {final_metrics.get(f'eval_precision_at_{k}', 0):.4f}")
        
        print("\nğŸŒŸ F1@K:")
        for k in [1, 3, 5, 10, 15]:
            print(f"  F1@{k:2d}: {final_metrics.get(f'eval_f1_at_{k}', 0):.4f}")
        
        print(f"\nâ­ MRR: {final_metrics.get('eval_mrr', 0):.4f}")
        print("="*50 + "\n")
        
        # ä¿å­˜ç»“æœ
        with open(os.path.join(self.config['output_dir'], "eval_results.json"), 'w') as f:
            json.dump(final_metrics, f, indent=2)
        
        with open(os.path.join(self.config['output_dir'], "metadata.pkl"), 'wb') as f:
            pickle.dump({
                'mlb': self.mlb,
                'all_reaction_names': self.all_reaction_names,
                'config': self.config,
                'eval_metrics': final_metrics,
            }, f)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return final_metrics


def main():
    print("=" * 50)
    print("ğŸ¯ äº¤å‰æ³¨æ„åŠ›æ¶æ„ (ç«¯åˆ°ç«¯å¤šæ ‡ç­¾åˆ†ç±»)")
    print("=" * 50)
    
    trainer = CrossAttentionTrainer(CROSSATT_CONFIG)
    trainer.train()


if __name__ == "__main__":
    main()